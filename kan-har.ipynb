{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import scipy.stats\nimport sklearn.model_selection\nimport tensorflow as tf\nimport itertools\nimport numpy as np\nimport scipy.interpolate\nimport glob\nimport re\nimport os\nimport pandas as pd\nimport os\nimport pickle\nimport scipy\nimport datetime\nimport numpy as np\n# Libraries for plotting\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_context('poster')\nimport sklearn.manifold\nimport requests\nimport zipfile\n#importing necessary libraries\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport scipy\nfrom scipy.stats import entropy\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn import preprocessing \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelEncoder,normalize, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,precision_score,recall_score  \nfrom sklearn.manifold import TSNE\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport optuna\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom collections import Counter\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.manifold import TSNE\n\n# import some libraries\nimport pandas as pd\nfrom sklearn.preprocessing import Normalizer,MinMaxScaler\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport random\nimport time\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelEncoder,normalize, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nimport shutil, time, os, requests, random, copy\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom collections import Counter\nimport joblib\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fvcore","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed_value):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    sklearn.random.seed(seed_value)\n    sklearn.utils.check_random_state(seed_value)\n    joblib.parallel_backend('threading', n_jobs=1)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set seed for reproducibility\nSEED = 42\nset_seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Download and extract zip file**","metadata":{}},{"cell_type":"code","source":"dataset_url = 'https://github.com/mmalekzadeh/motion-sense/blob/master/data/B_Accelerometer_data.zip?raw=true'\nworking_directory = \"/kaggle/working/\"\nr = requests.get(dataset_url, allow_redirects=True)\nwith open(working_directory + 'B_Accelerometer_data.zip', 'wb') as f:\n    f.write(r.content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with zipfile.ZipFile(working_directory + 'B_Accelerometer_data.zip', 'r') as zip_ref:\n    zip_ref.extractall(working_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Raw Data Processing func**","metadata":{}},{"cell_type":"code","source":"def process_motion_sense_accelerometer_files(accelerometer_data_folder_path):\n    \"\"\"\n    Preprocess the accelerometer files of the MotionSense dataset into the 'user-list' format\n    Data files can be found at https://github.com/mmalekzadeh/motion-sense/tree/master/data\n\n    Parameters:\n\n        accelerometer_data_folder_path (str):\n            the path to the folder containing the data files (unzipped)\n            e.g. motionSense/B_Accelerometer_data/\n            the trial folders should be directly inside it (e.g. motionSense/B_Accelerometer_data/dws_1/)\n\n    Return:\n        \n        user_datsets (dict of {user_id: [(sensor_values, activity_labels)]})\n            the processed dataset in a dictionary, of type {user_id: [(sensor_values, activity_labels)]}\n            the keys of the dictionary is the user_id (participant id)\n            the values of the dictionary are lists of (sensor_values, activity_labels) pairs\n                sensor_values are 2D numpy array of shape (length, channels=3)\n                activity_labels are 1D numpy array of shape (length)\n                each pair corresponds to a separate trial \n                    (i.e. time is not contiguous between pairs, which is useful for making sliding windows, where it is easy to separate trials)\n    \"\"\"\n\n    # label_set = {}\n    user_datasets = {}\n    all_trials_folders = sorted(glob.glob(accelerometer_data_folder_path + \"/*\"))\n\n    # Loop through every trial folder\n    for trial_folder in all_trials_folders:\n        trial_name = os.path.split(trial_folder)[-1]\n\n        # label of the trial is given in the folder name, separated by underscore\n        label = trial_name.split(\"_\")[0]\n        # label_set[label] = True\n        print(trial_folder)\n        \n        # Loop through files for every user of the trail\n        for trial_user_file in sorted(glob.glob(trial_folder + \"/*.csv\")):\n\n            # use regex to match the user id\n            user_id_match = re.search(r'(?P<user_id>[0-9]+)\\.csv', os.path.split(trial_user_file)[-1])\n            if user_id_match is not None:\n                user_id = int(user_id_match.group('user_id'))\n\n                # Read file\n                user_trial_dataset = pd.read_csv(trial_user_file)\n                user_trial_dataset.dropna(how = \"any\", inplace = True)\n\n                # Extract the x, y, z channels\n                values = user_trial_dataset[[\"x\", \"y\", \"z\"]].values\n\n                # the label is the same during the entire trial, so it is repeated here to pad to the same length as the values\n                labels = np.repeat(label, values.shape[0])\n\n                if user_id not in user_datasets:\n                    user_datasets[user_id] = []\n                user_datasets[user_id].append((values, labels))\n            else:\n                print(\"[ERR] User id not found\", trial_user_file)\n    \n    return user_datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Pre-Processing func**","metadata":{}},{"cell_type":"code","source":"def get_mode(np_array):\n    \n    \"\"\"\n    Get the mode (majority/most frequent value) from a 1D array\n    \"\"\"\n    frequency_dict = Counter(np_array)\n    max_key = max(frequency_dict, key=frequency_dict.get)\n#     return scipy.stats.mode(np_array)[0]\n    return max_key\n\n\ndef sliding_window_np(X, window_size, shift, stride, offset=0, flatten=None):\n    \"\"\"\n    Create sliding windows from an ndarray\n\n    Parameters:\n    \n        X (numpy-array)\n            The numpy array to be windowed\n        \n        shift (int)\n            number of timestamps to shift for each window\n            (200 here refers to 50% overlap, no overlap if =400)\n\n        stride (int)\n            stride of the window (dilation)\n\n        offset (int)\n            starting index of the first window\n        \n        flatten (function (array) -> (value or array) )\n            the function to be applied to a window after it is extracted\n            can be used with get_mode (see above) for extracting the label by majority voting\n            ignored if is None\n\n    Return:\n\n        Windowed ndarray\n            shape[0] is the number of windows\n    \"\"\"\n\n    overall_window_size = (window_size - 1) * stride + 1\n    num_windows = (X.shape[0] - offset - (overall_window_size)) // shift + 1\n    windows = []\n    for i in range(num_windows):\n        start_index = i * shift + offset\n        this_window = X[start_index : start_index + overall_window_size : stride]\n        if flatten is not None:\n            this_window = flatten(this_window)\n        windows.append(this_window)\n    return np.array(windows)\n\n\ndef get_windows_dataset_from_user_list_format(user_datasets, window_size=200, shift=200, stride=1, verbose=0):\n    \"\"\"\n    Create windows dataset in 'user-list' format using sliding windows\n\n    Parameters:\n\n        user_datasets\n            dataset in the 'user-list' format {user_id: [(sensor_values, activity_labels)]}\n        \n        window_size = 400\n            size of the window (output)\n\n        shift = 200\n            number of timestamps to shift for each window\n            (200 here refers to 50% overlap, no overlap if =400)\n\n        stride = 1\n            stride of the window (dilation)\n\n        verbose = 0\n            debug messages are printed if > 0\n\n    \n    Return:\n\n        user_dataset_windowed\n            Windowed version of the user_datasets\n            Windows from different trials are combined into one array\n            type: {user_id: ( windowed_sensor_values, windowed_activity_labels)}\n            windowed_sensor_values have shape (num_window, window_size, channels)\n            windowed_activity_labels have shape (num_window)\n\n            Labels are decided by majority vote\n    \"\"\"\n    \n    user_dataset_windowed = {}\n\n    for user_id in user_datasets:\n        if verbose > 0:\n            print(f\"Processing {user_id}\")\n        x = []\n        y = []\n\n        # Loop through each trail of each user\n        for v,l in user_datasets[user_id]:\n            v_windowed = sliding_window_np(v, window_size, shift, stride)\n            \n            # flatten the window by majority vote (1 value for each window)\n            l_flattened = sliding_window_np(l, window_size, shift, stride, flatten=get_mode)\n            if len(v_windowed) > 0:\n                x.append(v_windowed)\n                y.append(l_flattened)\n            if verbose > 0:\n                print(f\"Data: {v_windowed.shape}, Labels: {l_flattened.shape}\")\n\n        # combine all trials\n        user_dataset_windowed[user_id] = (np.concatenate(x), np.concatenate(y).squeeze())\n    return user_dataset_windowed\n\n\ndef combine_windowed_dataset(user_datasets_windowed, train_users, test_users=None, verbose=0):\n    \"\"\"\n    Combine a windowed 'user-list' dataset into training and test sets\n\n    Parameters:\n\n        user_dataset_windowed\n            dataset in the windowed 'user-list' format {user_id: ( windowed_sensor_values, windowed_activity_labels)}\n        \n        train_users\n            list or set of users (corresponding to the user_id) to be used as training data\n\n        test_users = None\n            list or set of users (corresponding to the user_id) to be used as testing data\n            if is None, then all users not in train_users will be treated as test users \n\n        verbose = 0\n            debug messages are printed if > 0\n\n    Return:\n        (train_x, train_y, test_x, test_y)\n            train_x, train_y\n                the resulting training/test input values as a single numpy array\n            test_x, test_y\n                the resulting training/test labels as a single (1D) numpy array\n    \"\"\"\n    \n    train_x = []\n    train_y = []\n    test_x = []\n    test_y = []\n    for user_id in user_datasets_windowed:\n        \n        v,l = user_datasets_windowed[user_id]\n        if user_id in train_users:\n            if verbose > 0:\n                print(f\"{user_id} Train\")\n            train_x.append(v)\n            train_y.append(l)\n        elif test_users is None or user_id in test_users:\n            if verbose > 0:\n                print(f\"{user_id} Test\")\n            test_x.append(v)\n            test_y.append(l)\n    \n\n    if len(train_x) == 0:\n        train_x = np.array([])\n        train_y = np.array([])\n    else:\n        train_x = np.concatenate(train_x)\n        train_y = np.concatenate(train_y).squeeze()\n    \n    if len(test_x) == 0:\n        test_x = np.array([])\n        test_y = np.array([])\n    else:\n        test_x = np.concatenate(test_x)\n        test_y = np.concatenate(test_y).squeeze()\n\n    return train_x, train_y, test_x, test_y\n\n\ndef get_mean_std_from_user_list_format(user_datasets, train_users):\n    \"\"\"\n    Obtain and means and standard deviations from a 'user-list' dataset (channel-wise)\n    from training users only\n\n    Parameters:\n\n        user_datasets\n            dataset in the 'user-list' format {user_id: [(sensor_values, activity_labels)]}\n        \n        train_users\n            list or set of users (corresponding to the user_ids) from which the mean and std are extracted\n\n    Return:\n        (means, stds)\n            means and stds of the particular users (channel-wise)\n            shape: (num_channels)\n\n    \"\"\"\n    \n    mean_std_data = []\n    for u in train_users:\n        for data, _ in user_datasets[u]:\n            mean_std_data.append(data)\n    mean_std_data_combined = np.concatenate(mean_std_data)\n    means = np.mean(mean_std_data_combined, axis=0)\n    stds = np.std(mean_std_data_combined, axis=0)\n    return (means, stds)\n\n\ndef normalise(data, mean, std):\n    \"\"\"\n    Normalise data (Z-normalisation)\n    \"\"\"\n\n    return ((data - mean) / std)\n\n\ndef apply_label_map(y, label_map):\n    \"\"\"\n    Apply a dictionary mapping to an array of labels\n    Can be used to convert str labels to int labels\n\n    Parameters:\n        y\n            1D array of labels\n        label_map\n            a label dictionary of (label_original -> label_new)\n\n    Return:\n        y_mapped\n            1D array of mapped labels\n            None values are present if there is no entry in the dictionary\n    \"\"\"\n\n    y_mapped = []\n    for l in y:\n        y_mapped.append(label_map.get(l))\n    return np.array(y_mapped)\n\n\ndef filter_none_label(X, y):\n    \"\"\"\n    Filter samples of the value None\n    Can be used to exclude non-mapped values from apply_label_map\n\n    Parameters:\n        X\n            data values\n\n        y\n            labels (1D)\n\n    Return:\n        (X_filtered, y_filtered)\n            X_filtered\n                filtered data values\n            \n            y_filtered\n                filtered labels (of type int)\n    \"\"\"\n\n    valid_mask = np.where(y != None)\n    return (np.array(X[valid_mask]), np.array(y[valid_mask], dtype=int))\n\n\ndef pre_process_dataset_composite(user_datasets, label_map, output_shape, train_users, test_users, window_size, shift, normalise_dataset=True, validation_split_proportion=0.2, verbose=0):\n    \"\"\"\n    A composite function to process a dataset\n    Steps\n        1: Use sliding window to make a windowed dataset (see get_windows_dataset_from_user_list_format)\n        2: Split the dataset into training and test set (see combine_windowed_dataset)\n        3: Normalise the datasets (see get_mean_std_from_user_list_format)\n        4: Apply the label map and filter labels (see apply_label_map, filter_none_label)\n        5: One-hot encode the labels (see tf.keras.utils.to_categorical)\n        6: Split the training set into training and validation sets (see sklearn.model_selection.train_test_split)\n    \n    Parameters:\n        user_datasets\n            dataset in the 'user-list' format {user_id: [(sensor_values, activity_labels)]}\n\n        label_map\n            a mapping of the labels\n            can be used to filter labels\n            (see apply_label_map and filter_none_label)\n\n        output_shape\n            number of output classifiction categories\n            used in one hot encoding of the labels\n            (see tf.keras.utils.to_categorical)\n\n        train_users\n            list or set of users (corresponding to the user_id) to be used as training data\n\n        test_users\n            list or set of users (corresponding to the user_id) to be used as testing data\n\n        window_size\n            size of the data windows\n            (see get_windows_dataset_from_user_list_format)\n\n        shift\n            number of timestamps to shift for each window\n            (see get_windows_dataset_from_user_list_format)\n\n        normalise_dataset = True\n            applies Z-normalisation if True\n\n        validation_split_proportion = 0.2\n            if not None, the proportion for splitting the full training set further into training and validation set using random sampling\n            (see sklearn.model_selection.train_test_split)\n            if is None, the training set will not be split - the return value np_val will also be none\n\n        verbose = 0\n            debug messages are printed if > 0\n\n    \n    Return:\n        (np_train, np_val, np_test)\n            three pairs of (X, y)\n            X is a windowed set of data points\n            y is an array of one-hot encoded labels\n\n            if validation_split_proportion is None, np_val is None\n    \"\"\"\n\n    # Step 1\n    user_datasets_windowed = get_windows_dataset_from_user_list_format(user_datasets, window_size=window_size, shift=shift)\n\n    # Step 2\n    train_x, train_y, test_x, test_y = combine_windowed_dataset(user_datasets_windowed, train_users)\n\n    # Step 3\n    if normalise_dataset:\n        means, stds = get_mean_std_from_user_list_format(user_datasets, train_users)\n        train_x = normalise(train_x, means, stds)\n        test_x = normalise(test_x, means, stds)\n\n    # Step 4\n    train_y_mapped = apply_label_map(train_y, label_map)\n    test_y_mapped = apply_label_map(test_y, label_map)\n\n    train_x, train_y_mapped = filter_none_label(train_x, train_y_mapped)\n    test_x, test_y_mapped = filter_none_label(test_x, test_y_mapped)\n\n    if verbose > 0:\n        print(\"Test\")\n        print(np.unique(test_y, return_counts=True))\n        print(np.unique(test_y_mapped, return_counts=True))\n        print(\"-----------------\")\n\n        print(\"Train\")\n        print(np.unique(train_y, return_counts=True))\n        print(np.unique(train_y_mapped, return_counts=True))\n        print(\"-----------------\")\n\n    # Step 5\n    train_y_one_hot = tf.keras.utils.to_categorical(train_y_mapped, num_classes=output_shape)\n    test_y_one_hot = tf.keras.utils.to_categorical(test_y_mapped, num_classes=output_shape)\n\n    r = np.random.randint(len(train_y_mapped))\n    assert train_y_one_hot[r].argmax() == train_y_mapped[r]\n    r = np.random.randint(len(test_y_mapped))\n    assert test_y_one_hot[r].argmax() == test_y_mapped[r]\n\n    # Step 6\n    if validation_split_proportion is not None and validation_split_proportion > 0:\n        train_x_split, val_x_split, train_y_split, val_y_split = sklearn.model_selection.train_test_split(train_x, train_y_one_hot, test_size=validation_split_proportion, random_state=42)\n    else:\n        train_x_split = train_x\n        train_y_split = train_y_one_hot\n        val_x_split = None\n        val_y_split = None\n        \n\n    if verbose > 0:\n        print(\"Training data shape:\", train_x_split.shape)\n        print(\"Validation data shape:\", val_x_split.shape if val_x_split is not None else \"None\")\n        print(\"Testing data shape:\", test_x.shape)\n\n    np_train = (train_x_split, train_y_split)\n    np_val = (val_x_split, val_y_split) if val_x_split is not None else None\n    np_test = (test_x, test_y_one_hot)\n\n    # original_np_train = np_train\n    # original_np_val = np_val\n    # original_np_test = np_test\n\n    return (np_train, np_val, np_test)\n\n\ndef pre_process_dataset_composite_in_user_format(user_datasets, label_map, output_shape, train_users, window_size, shift, normalise_dataset=True, verbose=0):\n    \"\"\"\n    A composite function to process a dataset which outputs processed datasets separately for each user (of type: {user_id: ( windowed_sensor_values, windowed_activity_labels)}).\n    This is different from pre_process_dataset_composite where the data from the training and testing users are not combined into one object.\n\n    Steps\n        1: Use sliding window to make a windowed dataset (see get_windows_dataset_from_user_list_format)\n        For each user:\n            2: Apply the label map and filter labels (see apply_label_map, filter_none_label)\n            3: One-hot encode the labels (see tf.keras.utils.to_categorical)\n            4: Normalise the data (see get_mean_std_from_user_list_format)\n    \n    Parameters:\n        user_datasets\n            dataset in the 'user-list' format {user_id: [(sensor_values, activity_labels)]}\n\n        label_map\n            a mapping of the labels\n            can be used to filter labels\n            (see apply_label_map and filter_none_label)\n\n        output_shape\n            number of output classifiction categories\n            used in one hot encoding of the labels\n            (see tf.keras.utils.to_categorical)\n\n        train_users\n            list or set of users (corresponding to the user_id) to be used for normalising the dataset\n\n        window_size\n            size of the data windows\n            (see get_windows_dataset_from_user_list_format)\n\n        shift\n            number of timestamps to shift for each window\n            (see get_windows_dataset_from_user_list_format)\n\n        normalise_dataset = True\n            applies Z-normalisation if True\n\n        verbose = 0\n            debug messages are printed if > 0\n\n    \n    Return:\n        user_datasets_processed\n            Processed version of the user_datasets in the windowed format\n            type: {user_id: (windowed_sensor_values, windowed_activity_labels)}\n    \"\"\"\n\n    # Preparation for step 2\n    if normalise_dataset:\n        means, stds = get_mean_std_from_user_list_format(user_datasets, train_users)\n\n    # Step 1\n    user_datasets_windowed = get_windows_dataset_from_user_list_format(user_datasets, window_size=window_size, shift=shift)\n\n    \n    user_datasets_processed = {}\n    for user, user_dataset in user_datasets_windowed.items():\n        data, labels = user_dataset\n\n        # Step 2\n        labels_mapped = apply_label_map(labels, label_map)\n        data_filtered, labels_filtered = filter_none_label(data, labels_mapped)\n\n        # Step 3\n        labels_one_hot = tf.keras.utils.to_categorical(labels_filtered, num_classes=output_shape)\n\n        # random check\n        r = np.random.randint(len(labels_filtered))\n        assert labels_one_hot[r].argmax() == labels_filtered[r]\n\n        # Step 4\n        if normalise_dataset:\n            data_filtered = normalise(data_filtered, means, stds)\n\n        user_datasets_processed[user] = (data_filtered, labels_one_hot)\n\n        if verbose > 0:\n            print(\"Data shape of user\", user, \":\", data_filtered.shape)\n    \n    return user_datasets_processed\n\n\ndef add_user_id_to_windowed_dataset(user_datasets_windowed, encode_user_id=True, as_feature=False, as_label=True, verbose=0):\n    \"\"\"\n    Add user ids as features or labels to a windowed dataset\n    The user ids are appended to the last dimension of the arrays\n    E.g. sensor values of shape (100, 400, 3) will become (100, 400, 4), and data[:, :, -1] will contain the user id\n    Similarly labels of shape (100, 5) will become (100, 6), and labels[:, -1] will contain the user id\n    \n    Parameters:\n        user_datasets_windowed\n            dataset in the 'windowed-user' format type: {user_id: (windowed_sensor_values, windowed_activity_labels)}\n\n        encode_user_id = True\n            whether to encode the user ids as integers\n            if True: \n                encode all user ids as integers when being appended to the np arrays\n                return the map from user id to integer as an output\n                note that the dtype of the output np arrays will be kept as float if they are originally of type float\n            if False:\n                user ids will be kept as is when being appended to the np arrays\n                WARNING: if the user id is of type string, the output arrays will also be converted to type string, which might be difficult to work with\n\n        as_feature = False\n            user ids will be added to the windowed_sensor_values arrays as extra features if True\n\n        as_label = False\n            user ids will be added to the windowed_activity_labels arrays as extra labels if True\n\n        verbose = 0\n            debug messages are printed if > 0\n\n    Return:\n        user_datasets_modified, user_id_encoder\n\n            user_datasets_modified\n                the modified version of the input (user_datasets_windowed)\n                with the same type {user_id: ( windowed_sensor_values, windowed_activity_labels)}\n            user_id_encoder\n                the encoder which maps user ids to integers\n                type: {user_id: encoded_user_id}\n                None if encode_user_id is False\n    \"\"\"\n\n    # Create the mapping from user_id to integers\n    if encode_user_id:\n        all_users = sorted(list(user_datasets_windowed.keys()))\n        user_id_encoder = dict([(u, i) for i, u in enumerate(all_users)])\n    else:\n        user_id_encoder = None\n\n    # if none of the options are enabled, return the input\n    if not as_feature and not as_label:\n        return user_datasets_windowed, user_id_encoder\n\n    user_datasets_modified = {}\n    for user, user_dataset in user_datasets_windowed.items():\n        data, labels = user_dataset\n\n        # Get the encoded user_id\n        if encode_user_id:\n            user_id = user_id_encoder[user]\n        else:\n            user_id = user\n\n        # Add user_id as an extra feature\n        if as_feature:\n            user_feature = np.expand_dims(np.full(data.shape[:-1], user_id), axis=-1)\n            data_modified = np.append(data, user_feature, axis=-1)\n        else:\n            data_modified = data\n        \n        # Add user_id as an extra label\n        if as_label:\n            user_labels = np.expand_dims(np.full(labels.shape[:-1], user_id), axis=-1)\n            labels_modified = np.append(labels, user_labels, axis=-1)\n        else:\n            labels_modified = labels\n\n        if verbose > 0:\n            print(f\"User {user}: id {repr(user)} -> {repr(user_id)}, data shape {data.shape} -> {data_modified.shape}, labels shape {labels.shape} -> {labels_modified.shape}\")\n\n        user_datasets_modified[user] = (data_modified, labels_modified)\n    \n    return user_datasets_modified, user_id_encoder\n\n\ndef make_batches_reshape(data, batch_size):\n    \"\"\"\n    Make a batched dataset from a windowed time-series by simple reshaping\n    Note that the last batch is dropped if incomplete\n\n    Parameters:\n        data\n            A 3D numpy array in the shape (num_windows, window_size, num_channels)\n\n        batch_size\n            the (maximum) size of the batches\n\n    Returns:\n        batched_data\n            A 4D numpy array in the shape (num_batches, batch_size, window_size, num_channels)\n    \"\"\"\n\n    max_len = (data.shape[0]) // batch_size * batch_size\n    return data[:max_len].reshape((-1, batch_size, data.shape[-2], data.shape[-1]))\n\n\ndef np_random_shuffle_index(length):\n    \"\"\"\n    Get a list of randomly shuffled indices\n    \"\"\"\n    indices = np.arange(length)\n    np.random.shuffle(indices)\n    return indices\n\n\ndef ceiling_division(n, d):\n    \"\"\"\n    Ceiling integer division\n    \"\"\"\n    return -(n // -d)\n\n\ndef get_batched_dataset_generator(data, batch_size):\n    \"\"\"\n    Create a data batch generator\n    Note that the last batch might not be full\n\n    Parameters:\n        data\n            A numpy array of data\n\n        batch_size\n            the (maximum) size of the batches\n\n    Returns:\n        generator<numpy array>\n            a batch of the data with the same shape except the first dimension, which is now the batch size\n    \"\"\"\n\n    num_bathes = ceiling_division(data.shape[0], batch_size)\n    for i in range(num_bathes):\n        yield data[i * batch_size : (i + 1) * batch_size]\n\n    # return data[:max_len].reshape((-1, batch_size, data.shape[-2], data.shape[-1]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_save_path = working_directory\nif not os.path.exists(working_directory):\n    os.mkdir(working_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accelerometer_data_folder_path = working_directory + 'B_Accelerometer_data'\n# accelerometer_data_folder_path = \"/kaggle/working/B_Accelerometer_data\"\nuser_datasets = process_motion_sense_accelerometer_files(accelerometer_data_folder_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(working_directory + 'motion_sense_user_split.pkl', 'wb') as f:\n    pickle.dump({\n        'user_split': user_datasets,\n    }, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **pre processing**","metadata":{}},{"cell_type":"code","source":"# Parameters\nwindow_size = 500\ninput_shape = (window_size, 3)\n\n# Dataset Metadata \ntransformation_multiple = 1\ndataset_name = 'motion_sense.pkl'\ndataset_name_user_split = 'motion_sense_user_split.pkl'\n\nlabel_list = ['null', 'sit', 'std', 'wlk', 'ups', 'dws', 'jog']\nlabel_list_full_name = ['null', 'sitting', 'standing', 'walking', 'walking upstairs', 'walking downstairs', 'jogging']\nhas_null_class = True\n\nlabel_map = dict([(l, i) for i, l in enumerate(label_list)])\n\noutput_shape = len(label_list)\n\nmodel_save_name = f\"motionsense_acc\"\n\nsampling_rate = 50.0\nunit_conversion = scipy.constants.g\n\n# a fixed user-split\ntest_users_fixed = [1, 14, 19, 23, 6]\ndef get_fixed_split_users(har_users):\n#     print(har_users)\n#     test_users = har_users[0::5]\n    test_users = test_users_fixed\n    train_users = [u for u in har_users if u not in test_users]\n#     train_users = [10, 11, 12, 13, 15, 16, 17, 18, 2, 20, 21, 22, 24, 3, 4, 5, 7, 8]\n#     test_users = [1, 14, 19, 23, 6, 9]\n    return (train_users, test_users)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_name_user_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_save_path = \"/kaggle/working/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(dataset_save_path + dataset_name_user_split, 'rb') as f:\n    dataset_dict = pickle.load(f)\n    user_datasets = dataset_dict['user_split']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"har_users = list(user_datasets.keys())\ntrain_users, test_users = get_fixed_split_users(har_users)\nprint(f'Testing: {test_users}, Training: {train_users}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np_train, np_val, np_test = pre_process_dataset_composite(\n    user_datasets = user_datasets,\n    label_map = label_map,\n    output_shape = output_shape,\n    train_users = train_users,\n    test_users = test_users,\n    window_size = window_size,\n    shift = window_size // 2,\n    normalise_dataset = True,\n    verbose = 1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = np.vstack((np_train[0], np_val[0]))\ntrain_label = np.vstack((np_train[1], np_val[1]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_label.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np_train=(train_data, train_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np_train[0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import entropy\n\ndef calculate_rms(data):\n    \"\"\"Compute Root Mean Squared (RMS)\"\"\"\n    return np.sqrt(np.mean(data**2, axis=1))\n\ndef calculate_p2p(data):\n    \"\"\"Compute Peak-to-Peak\"\"\"\n    return np.abs(np.max(data, axis=1)) + np.abs(np.min(data, axis=1))\n\ndef calculate_entropy(data, bins=500):\n    \"\"\"Compute Shannon Entropy after binning\"\"\"\n    ent = np.zeros((data.shape[0], data.shape[2]))  # (N, 3)\n    for i in range(data.shape[2]):  # Iterate over channels\n        counts = np.apply_along_axis(lambda x: np.histogram(x, bins=bins)[0], 1, data[:, :, i])\n        ent[:, i] = entropy(counts, axis=1)\n    return ent\n\ndef calculate_clearance(data):\n    \"\"\"Compute Clearance Factor\"\"\"\n    return (np.mean(np.sqrt(np.abs(data)), axis=1))**2\n\ndef extract_time_features(data):\n    \"\"\"Extracts 12 features per channel (3 channels) -> Output shape (N, 36)\"\"\"\n    N, _, C = data.shape  # (N, 200, 3)\n    \n    mean_abs = np.mean(np.abs(data), axis=1)\n    std_dev = np.std(data, axis=1)\n    skewness = np.mean(((data - np.mean(data, axis=1, keepdims=True)) / std_dev[:, np.newaxis, :])**3, axis=1)\n    kurt = np.mean(((data - np.mean(data, axis=1, keepdims=True)) / std_dev[:, np.newaxis, :])**4, axis=1) - 3\n    rms = calculate_rms(data)\n    max_abs = np.max(np.abs(data), axis=1)\n    p2p = calculate_p2p(data)\n    crest = max_abs / rms\n    clearance = calculate_clearance(data)\n    shape = rms / mean_abs\n    impulse = max_abs / mean_abs\n    ent = calculate_entropy(data)\n\n    # Stack all features along the last dimension\n    features = np.hstack([mean_abs, std_dev, skewness, kurt, ent, rms, max_abs, p2p, crest, clearance, shape, impulse])  # (N, 36)\n    \n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_train = extract_time_features(np_train[0])\nfeatures_test = extract_time_features(np_test[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.argmax(np_train[1], axis=1).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_train = np.argmax(np_train[1], axis=1)\nlabels_test  = np.argmax(np_test[1], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_train = labels_train - 1\nlabels_test = labels_test - 1\nCounter(labels_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(features_train)\n\nx_train_standardized = scaler.transform(features_train)\nx_test_standardized = scaler.transform(features_test)\n\nprint(np.mean(x_train_standardized), np.mean(x_test_standardized))\nprint(np.std(x_train_standardized), np.std(x_test_standardized))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.size(-1) == self.in_features\n        original_shape = x.shape\n        x = x.view(-1, self.in_features)\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        output = base_output + spline_output\n        \n        output = output.view(*original_shape[:-1], self.out_features)\n        return output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        \"\"\"\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        \"\"\"\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n\n\nclass KAN(torch.nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KAN, self).__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        self.layers = torch.nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n            self.layers.append(\n                KANLinear(\n                    in_features,\n                    out_features,\n                    grid_size=grid_size,\n                    spline_order=spline_order,\n                    scale_noise=scale_noise,\n                    scale_base=scale_base,\n                    scale_spline=scale_spline,\n                    base_activation=base_activation,\n                    grid_eps=grid_eps,\n                    grid_range=grid_range,\n                )\n            )\n\n    def forward(self, x: torch.Tensor, update_grid=False):\n        for layer in self.layers:\n            if update_grid:\n                layer.update_grid(x)\n            x = layer(x)\n        return x\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        return sum(\n            layer.regularization_loss(regularize_activation, regularize_entropy)\n            for layer in self.layers\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Create_Dataloader_final1(x_train, x_test, y_train, y_test, batch_size = 8):\n    \n    X_train, X_valid, y_train, y_valid = x_train, x_test, y_train, y_test\n\n    trainX = np.array(X_train)\n    y_train = np.array(y_train)\n    validX = np.array(X_valid)\n    y_valid = np.array(y_valid)\n    \n    \n    # reshaping data\n    X_train1 = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n    X_valid1 = np.reshape(validX, (validX.shape[0],validX.shape[1],1))\n\n    # Create the DataLoader for our data\n    X_train1 = torch.tensor(X_train1)\n    X_valid1 = torch.tensor(X_valid1)\n    \n    y_train = torch.tensor(y_train).type(torch.LongTensor)\n    y_valid = torch.tensor(y_valid).type(torch.LongTensor)\n\n    train_data = TensorDataset(X_train1, y_train)\n    valid_data = TensorDataset(X_valid1, y_valid)\n\n    train_sampler = RandomSampler(train_data)\n    valid_sampler = RandomSampler(valid_data)\n\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)\n    valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size, drop_last=True)\n    return train_dataloader, valid_dataloader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def final_evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    val_ep_loss = []\n    val_ep_acc = []\n    val_ep_f1=[]\n    val_ep_precision=[]\n    val_ep_recall=[]\n\n    model.eval()\n    loss_sublist = np.array([])\n    acc_sublist = np.array([])\n    f1_sublist = np.array([])\n    precision_sublist = np.array([])\n    recall_sublist = np.array([])\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for x,y in val_dataloader:\n\n            z = model(x.to(device).float())\n            val_loss = loss_fn(z,y.to(device).squeeze())\n\n            # model's prediction\n            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n            f1_sublist = np.append(f1_sublist,f1_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n            precision_sublist = np.append(precision_sublist,precision_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n            recall_sublist = np.append(recall_sublist,recall_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n\n    return np.mean(loss_sublist), np.mean(acc_sublist), np.mean(f1_sublist), np.mean(precision_sublist), np.mean(recall_sublist)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def final_train(model, optimizer, ep, train_dataloader_final, val_dataloader_final, evaluation=False):\n    \"\"\"Train the final model.\n    \"\"\"\n\n    # Start training loop\n    print(\"Start training...\\n\")\n    tr_ep_loss = []\n    tr_ep_acc = []\n    tr_ep_f1 = []\n    tr_ep_precision = []\n    tr_ep_recall = []\n\n    val_ep_loss = []\n    val_ep_acc = []\n    val_ep_f1 = []\n    val_ep_precision = []\n    val_ep_recall = []\n\n    best_val_f1 = 0\n    loss_fn = nn.CrossEntropyLoss()\n\n    for e in range(1, ep + 1):\n        # =======================================\n        #               Training\n        # =======================================\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n        \n        # For each batch of training data...\n        loss_sublist = np.array([])\n        acc_sublist = np.array([])\n        f1_sublist = np.array([])\n        precision_sublist = np.array([])\n        recall_sublist = np.array([])\n\n        for batch, (x, y) in enumerate(train_dataloader_final):\n#             x.to(device)\n#             y.to(device)\n            batch_counts +=1\n            optimizer.zero_grad()\n\n            # positive pair, with encoding\n            z = model_final(x.to(device).float())\n            #y_true.append(y.squeeze(1).numpy())\n\n            # calculate loss\n            loss = loss_fn(z, y.to(device).squeeze())\n            loss.backward()\n\n            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n            loss_sublist = np.append(loss_sublist, loss.cpu().data)\n            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n            f1_sublist = np.append(f1_sublist,f1_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n            precision_sublist = np.append(precision_sublist,precision_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n            recall_sublist = np.append(recall_sublist,recall_score(np.array(y.data.view(-1).reshape(-1,1)),np.array((np.argmax(preds,axis=1)).reshape(-1,1)),average=\"macro\",zero_division=0))\n\n            optimizer.step()\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Reset batch tracking variables\n        batch_loss, batch_counts = 0, 0\n        t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = np.mean(loss_sublist)\n        tr_ep_loss.append(avg_train_loss)\n        \n        total_accuracy = np.mean(acc_sublist)\n        tr_ep_acc.append(total_accuracy)\n\n        total_f1 = np.mean(f1_sublist)\n        tr_ep_f1.append(total_f1)\n        \n        total_precision = np.mean(precision_sublist)\n        tr_ep_precision.append(total_precision)\n        \n        total_recall = np.mean(recall_sublist)\n        tr_ep_recall.append(total_recall)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy, val_f1, val_precision, val_recall = final_evaluate(model, val_dataloader_final)\n            \n            if val_f1>best_val_f1:\n                best_model_final1=copy.deepcopy(model)\n                model_name_location = f\"/kaggle/working/best_model_mafaulda.pt\"\n                torch.save(best_model_final1.state_dict(), model_name_location)\n                best_val_f1 = val_f1\n                print(\"This is best!\")\n                \n            val_ep_loss.append(val_loss)\n            val_ep_acc.append(val_accuracy)\n            val_ep_f1.append(val_f1)\n            val_ep_precision.append(val_precision)\n            val_ep_recall.append(val_recall)\n\n            time_elapsed = time.time() - t0_epoch\n\n            # Print the header of the result table\n            print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Accuracy':^12} | {'Train F1 score':^12} |{'Train Precision score':^12} |{'Train Recall score':^12} | {'Val Loss':^12} | {'Val Accuuracy':^12} | {'Val F1 score':^12}|{'Val Precision score':^12}|{'Val Recall score':^12}| {'Elapsed':^9}\")\n            print(\"-\"*81)\n            print(f\"{e:^7} | {avg_train_loss:^12.6f} | {total_accuracy:^14.6} | {total_f1:^14.6} | {total_precision:^14.6} |{total_recall:^14.6} |{val_loss:^12.6f} | {val_accuracy:^14.6f} | {val_f1:^14.6f}|{val_precision:^14.6f}|{val_recall:^14.6f}| {time_elapsed:^9.2f}\")\n            print(\"-\"*81)\n        print(\"\\n\")\n\n    torch.save(best_model_final1.state_dict(), '/kaggle/working/final_model_mafaulda_1.pt')\n    \n    return best_val_f1\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda:0\"\n# device = \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KAN_model(nn.Module):\n\n    def __init__(self):\n        super(KAN_model, self).__init__()\n        self.KAN1 = KAN([36, 100], grid_size=10, spline_order=1, grid_range=[-1, 4])\n        self.projection = KAN([100, 6], grid_size=10, spline_order=1, grid_range=[-1, 1])\n\n    def forward(self, x):\n        x = x.view(x.shape[0], 1,-1)\n        out = self.KAN1(x)\n        out = self.projection(out)\n        out_f = out.view(x.shape[0], out.size(1) * out.size(2))\n        return out_f","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_final = KAN_model()\nmodel_final.to(device)\noptimizer = torch.optim.AdamW(model_final.parameters(), lr=0.004, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.05, amsgrad=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 800\n\ntrain_dataloader_final, val_dataloader_final = Create_Dataloader_final1(x_train_standardized, x_test_standardized, labels_train, labels_test, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_parameters = filter(lambda p: p.requires_grad, model_final.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nparams","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #final training\nepoch = 200\nfinal_train(model_final, optimizer, epoch, train_dataloader_final, val_dataloader_final, evaluation=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model_path = \"/kaggle/working/best_model_mafaulda.pt\"\n\nbest_model = KAN_model()\nbest_model.load_state_dict(torch.load(best_model_path))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Step 1: Run inference to get predictions\ny_true = []\ny_pred = []\n\nbest_model.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_model.to(device)\nbest_model.float()\n\nwith torch.no_grad():\n    for data, target in val_dataloader_final:  # assuming (data, label)\n        data, target = data.to(device), target.to(device)\n        data = data.float()\n        outputs = best_model(data)\n        preds = torch.argmax(outputs, dim=1)\n        y_true.extend(target.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\n# Step 2: Compute confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Optional: class names (if known, replace with actual class names)\nclass_names = ['DownStairs', 'Jogging', 'Sitting', 'Standing', 'UpStairs', 'Walking']\n\n# Step 3: Plot with seaborn\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\n\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.tight_layout()\n# plt.show()\nplt.savefig(\"conf_matrix.eps\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}